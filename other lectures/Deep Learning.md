# 第三章中的ideas

随机梯度下降：抽取一个小批量计算梯度，并更新参数。

均方差损失函数和最大似然估计之间的联系：如果噪声符合正态分布，即误差$\epsilon$符合${1\over \sqrt{2\pi}\sigma}\exp(-{\epsilon^2\over 2\sigma^2})$

取对数之后就比较明显了。

交叉熵的概念：$\sum -p(j)\log q(j)$ $p(j)$是真实分布，而$q(j)$是预测结果概率。我们用这个函数，可以衡量其准确度。

Softmax函数用于将所有概率归一化。但在我看来，这个归一化函数是否合理，有待商榷。

# 第四章中的ideas

目前常用的激活函数有Sigmoid，tanh，ReLU。但是目前Sigmoid基本不被使用。因为Sigmoid的导数不够大，尤其是远离0时。层数过多时会引起梯度消失。与Sigmoid不同的是，tanh函数是奇函数。导数也存在类似问题。所以现在多用ReLU。

如果网络中没有激活函数层，那么就相当于矩阵相乘而已。堆叠稠密层没有价值。

过拟合的问题，目前主要有两种解决方法。第一种，是引入正则化损失。目的是为了避免某个参数过大，导致某个节点的影响力过高。第二种，是使用暂退法。在反向传播时，故意停止某几个节点的反向传播。这样的目的是为了提高泛化的准确性。因为一部分节点“不存在”，不会影响最终的结果。

正则化损失使用L2范数，在线性回归中就是岭回归；而使用L1范数，则就是套索回归。

前向传播，实际上就是计算的过程。反向传播，则是按照微积分的链式法则进行计算。求梯度的过程是从后向前的，故名之。

梯度爆炸的问题，则是由于网络初始化数值不恰当，导致梯度过大，结果不收敛。

对称性导致神经网络表达性下降的问题：如果某层中（只是举例！）两个神经元计算得到的梯度始终相等，那么这两个神经元的实际作用就相当于一个神经元。我们要用暂退法，正则化来打破这种对称性。

我们可以通过参数初始化的方式，来缓解上述的几个问题。

对于非高难度问题，随机参数初始化即可。或者我们可以使用Xavier初始化。

训练数据对结果的影响：协变量的偏移，标签的偏移和概念的偏移。因此，我们必须要有所动作，想办法减小这种偏移的影响。

# 第六章中的ideas

图像中的不变性：对于物体的识别，应该具有**平移不变性**。同时，在神经网络的前面几层中，应该也具有**局部性**。显然，离得很远的图像之间应当关联较小。

准确地说，卷积运算应该称为互相关。有时也可以喊它特征映射，因为它如同提取出了特征一般。感受野，指的是在前向传播期间可能影响传播的所有元素。

padding和stride，前者为了保持信息量，后者则为了减少冗余。

卷积核通常会设置成奇数，这是为了保证图像两侧处理时的对称性。

通过增加输出通道的方法，可以认为是提取出了不同的特征，进而把这些特征从不同的通道中进行了处理。

1*1卷积层：利用不同的核函数，把不同的特征类型提取到不同的通道中。

池化层的作用：将不同通道的特征提取到同一个神经元中，从而提高图像的识别力。

一种思路是最大池化，用最大的特征值代表全部。另一种思路是使用平均池化，将特征值取平均，从而达到普遍的识别。这样的主要优点是减少卷积层对位置的敏感性。

VGG块： 一堆3*3的卷积层，最后放一个步幅为2的3 *3最大池化层，就是模块化。

NiN块：将全连接层换为两个1*1的卷积层来代替全连接层。在使用时交替使用NiN块以及步幅为2的最大池化层。在最后时使用全局平均池化。

GoogLeNet：含并行连接的网络。Inception块：在同一个块中，有多条通路，从而抽取更多的信息。在不同的通路中，使用了不同尺寸的卷积核。从另一个方面去想，这也是在提取不同大小的信息。

批量规范化：对于一个小输入，我们将中间层的输入输出进行标准化。因为在不同的层之间，它们的数据量级可能有较为显著的区别。表达式为$\gamma\circ {x-\hat\mu\over\sigma}+\beta$。$\gamma,\beta$也是需要学习的参数。为了保证不发生除零错误，我们需要给$\sigma$始终加上一个常量。虽然引入了噪声，但是泛化效果更好。（看上去有点玄学。但现实情况里也确实一直有噪声，也能理解）

批量规范化，在全连接层中的实现，一般放在激活函数之前。卷积层也放在相似的位置上，但是每个层应当拥有自己的拉伸参数和偏移参数。在torch中，我们可以在放置全连接层、卷积层的时候在参数中进行设置(batchnorm)。在预测时，我们用总体的均值和方差来参与计算。

“将现代深度学习的实践比作炼金术” 

楽。现在AI的工作是不是很大程度上在依靠着直觉？

我喜欢依靠直觉，但我不一定有直觉XD

残差层：我们可以把优化模型的过程，看做寻找最优函数的过程。但是，这个最优函数并**不一定**可以被我们的架构取到。所以，我们希望造出一个更好的架构，让它中距离最优函数的“距离”更近些。如果我们能够严格保证我们创新的架构与原先的架构模拟的内容存在严格的包含关系，那么我们就可以认为新的架构确实离目标函数更近些。因此，产生了残差块。残差块的目的是，使得模拟的函数从原先的$f(x)$转变为$f(x)-x$,(因为在输出时还会加上$x$)

稠密块：首先由一串卷积层构成。在每经过一次卷积层后，将输出的结果与输入连接，然后再进入下一层的运算。思想是为了如Taylor展开般，更好地去模拟一个函数。因此，在最后的输出中，通道数会显著增加。因此，要在之后添加一个过渡层，其由1*1卷积层和平均池化层构成，从而降低模型复杂度。

# 第八章中的ideas

我们应当如何处理文本？显然，要把大象放进冰箱，我们要先打开冰箱门，将文本作为字符串加载到内存中。然后，我们将字符串拆分为词元，比如单词和字符。然后，建立一个词表，把词元映射到数字。最后，将文本转换为数字索引序列。

对于语言模型，我们的目的实际上是为了估计$P(x_1,...,x_T)$这一联合概率。那么，对于语言模型的生成问题，我们实际上就是想要找到一个合适的单词，使得$P(x_t|x_{t-1},...,x_1)$有一个较为可观的值。

一种直观的想法，就是统计某一个单词在数据库中出现的总次数，然后分别统计其前一个字母为某一值的总次数。但是，这种方法的精确度很低，因为对于一些不常见的单词组合，不一定找得出合适数量的估计。

一种方法是采用拉普拉斯平滑的方式。它的假设是所有词都会等可能出现。

$$
\hat P(x)={n(x)+\epsilon\over n + m\epsilon}
$$

$n$为总单词数，$m$为出现的不同单词有多少个，$\epsilon$是一个规定的超参数。

如果我们把文本中的词汇做出一些统计后，我们可以看到出现次数最多的词是诸如the、and这类没营养的词。是故，我们想过滤掉这些词，我们把这些词汇称为停用词（stop words）。在过滤掉最前面最高词频的一些词汇后，剩余词汇的出现频率基本满足齐普夫定律。即，第$i$高词频的词出现次数有$n_i\propto {1\over i^\alpha}$

拉普拉斯平滑的颓势于此尽见。从直觉上去想，我们过分高估了尾部单词的出现频率。

除了一元语法词，单词序列基本上也遵循齐普夫定律。

我们如果打算使用神经网络来训练语言模型，那么我们应当对数据进行一定的预处理。我们会把长序列划分为一个个小的子序列。一般有两种采样的方式：随机采样和顺序采样。

我们为什么需要循环神经网络？这是因为如果单词的数量增加，如果我们考虑条件概率，那么我们需要存储的参数数量显然会以指数级别增加。这是不可接受的。因此，我们将过去对现在的影响抽象为一个隐状态。

并且，隐状态之间的转移方程可以抽象为$h_t=f(x_t,h_{t-1})$。我们将每个单词出现的概率近似表示为$P(x_t|x_{t-1},...,x_1)\approx P(x_t|h_{t-1})$

隐状态可以用这个方式计算：$H_t=\phi(XW_1+H_{t-1}W_2+b_1)$

$H_t$表征了从$x_1$到$x_t$之间的所有变量的影响。最终的输出，我们可以记作$O_t=H_tW_3+b_2$

独热编码：将无关变量之间由于编码产生的”连续“分离开来。

梯度裁剪：解决梯度爆炸的问题。即，对于得到的梯度，我们每次反向移动的梯度量为$\min(1,\frac{\theta}{||\vec g||})\vec g$

但是，考虑到循环神经网络中，最后的梯度与每个时刻的值都有关。如果我们对梯度进行完全计算，这在时空上是不可接受的。一般而言，我们只会计算最后几次影响下的梯度，使用这种定长梯度的方法来解决这一问题。还有一种处理方式是随机选取一个长度，选择这些对象影响下的梯度。目前，选取定长的这种做法较为主流。

# 第九章中的ideas

GRU：门控循环单元。是RNN下的一种特殊变体。实际上，我们新增了两种结构，更新门和重置门。在具体实现上，我们在RNN基础上，每次计算出的结构与重置矩阵进行一次元素积，重置矩阵中的取值在0~1之间。值越趋于0，就越会被重置。我们将结果进行偏置等操作后，输出的结果为$Z\circ H_t+(1-Z)\circ H_{t-1}$也就是说，将当前的结果视作两次操作的混合，或者可以把这看做是更新的程度。我们把重置门操作后的情况，称为候选隐状态。将更新门操作后的情况，称为隐状态。

LSTM：长短期循环单元。在这种结构下，我们需要三种门，输入门，遗忘门，输出门。三种门的运算都符合RNN的基本操作，即基于之前的隐状态，进行矩阵乘法以及偏置加法。候选记忆元的计算与前三者无异。其思想与GRU非常接近。区别在于，其将$1-Z$转化为了$I$（输入门）进行运算。之后，在类似于GRU的操作过后，我们使用激活函数进行处理，然后将其与输出门进行元素乘法。因此，我们可以认为LSTM和GRU的区别在于，把三次处理的参数之间的相关性去除，从而提供更高的准确度。这样做可以缓解梯度爆炸和梯度消失的问题。(?)

深度循环神经网络：堆叠多个“并行线”的RNN，RNN中的每个单元不仅依赖于时序上的上一单元。

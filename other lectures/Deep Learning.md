# 第三章中的ideas

随机梯度下降：抽取一个小批量计算梯度，并更新参数。

均方差损失函数和最大似然估计之间的联系：如果噪声符合正态分布，即误差$\epsilon$符合${1\over \sqrt{2\pi}\sigma}\exp(-{\epsilon^2\over 2\sigma^2})$

取对数之后就比较明显了。

交叉熵的概念：$\sum -p(j)\log q(j)$ $p(j)$是真实分布，而$q(j)$是预测结果概率。我们用这个函数，可以衡量其准确度。

Softmax函数用于将所有概率归一化。但在我看来，这个归一化函数是否合理，有待商榷。

# 第四章中的ideas

目前常用的激活函数有Sigmoid，tanh，ReLU。但是目前Sigmoid基本不被使用。因为Sigmoid的导数不够大，尤其是远离0时。层数过多时会引起梯度消失。与Sigmoid不同的是，tanh函数是奇函数。导数也存在类似问题。所以现在多用ReLU。

如果网络中没有激活函数层，那么就相当于矩阵相乘而已。堆叠稠密层没有价值。

过拟合的问题，目前主要有两种解决方法。第一种，是引入正则化损失。目的是为了避免某个参数过大，导致某个节点的影响力过高。第二种，是使用暂退法。在反向传播时，故意停止某几个节点的反向传播。这样的目的是为了提高泛化的准确性。因为一部分节点“不存在”，不会影响最终的结果。

正则化损失使用L2范数，在线性回归中就是岭回归；而使用L1范数，则就是套索回归。

前向传播，实际上就是计算的过程。反向传播，则是按照微积分的链式法则进行计算。求梯度的过程是从后向前的，故名之。

梯度爆炸的问题，则是由于网络初始化数值不恰当，导致梯度过大，结果不收敛。

对称性导致神经网络表达性下降的问题：如果某层中（只是举例！）两个神经元计算得到的梯度始终相等，那么这两个神经元的实际作用就相当于一个神经元。我们要用暂退法，正则化来打破这种对称性。

我们可以通过参数初始化的方式，来缓解上述的几个问题。

对于非高难度问题，随机参数初始化即可。或者我们可以使用Xavier初始化。

训练数据对结果的影响：协变量的偏移，标签的偏移和概念的偏移。因此，我们必须要有所动作，想办法减小这种偏移的影响。

# 第六章中的ideas

图像中的不变性：对于物体的识别，应该具有**平移不变性**。同时，在神经网络的前面几层中，应该也具有**局部性**。显然，离得很远的图像之间应当关联较小。

准确地说，卷积运算应该称为互相关。有时也可以喊它特征映射，因为它如同提取出了特征一般。感受野，指的是在前向传播期间可能影响传播的所有元素。

padding和stride，前者为了保持信息量，后者则为了减少冗余。

卷积核通常会设置成奇数，这是为了保证图像两侧处理时的对称性。

通过增加输出通道的方法，可以认为是提取出了不同的特征，进而把这些特征从不同的通道中进行了处理。

1*1卷积层：利用不同的核函数，把不同的特征类型提取到不同的通道中。

池化层的作用：将不同通道的特征提取到同一个神经元中，从而提高图像的识别力。

一种思路是最大池化，用最大的特征值代表全部。另一种思路是使用平均池化，将特征值取平均，从而达到普遍的识别。这样的主要优点是减少卷积层对位置的敏感性。

VGG块： 一堆3*3的卷积层，最后放一个步幅为2的3 *3最大池化层，就是模块化。

NiN块：将全连接层换为两个1*1的卷积层来代替全连接层。在使用时交替使用NiN块以及步幅为2的最大池化层。在最后时使用全局平均池化。

GoogLeNet：含并行连接的网络。Inception块：在同一个块中，有多条通路，从而抽取更多的信息。在不同的通路中，使用了不同尺寸的卷积核。从另一个方面去想，这也是在提取不同大小的信息。

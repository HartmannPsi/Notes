# 第一章 初探强化学习

强化学习，解决的是序贯决策的问题。目标是要找到一个最优的占用度量。占用度量，可以理解为在不同情况下，采取不同策略的概率分布模型。

# 第二章 多臂老虎机(MAB)问题

有一台老虎机，其有k根拉杆，每根拉杆对应不同的奖励概率分布。如何在操作T次拉杆后，获得尽量高的奖励呢？ 

我们引入懊悔(regret)这一概念来刻画拉动一根拉杆的期望奖励和最优拉杆期望之间的差距，定义为二者之差。我们的目标，就是要最小化懊悔。显然，我们需要不断拉一根拉杆，才能对它产生的值有一个大概的估计。我们使用增量式估计来更新，而非重新计算。递推的复杂度更低。

操作实际上可以分为两种类型：探索和利用。

## $\epsilon-$贪婪算法

每次有$1-\epsilon$的概率拉下期望奖励最大的拉杆，否则，就随机选择一根拉杆。$\epsilon$可以设置为随着时间下降。

## 上置信界算法

显然，如果我们对一根拉杆测试较多，那么它的确认度较高，我们更会相信目前的结果是可靠的。原理依靠于霍夫丁不等式，即：

$$
P(E[X]\geq\bar x_n+u)\leq e^{-2nu^2}
$$

我们假设这是第t次操作，拉下的是k号拉杆，那么不确定度为$\hat U(a_t)=\sqrt{\frac{-\log p}{2N(a_t)}}=\sqrt{\frac{-\log p}{2N(k)}}$

根据霍夫丁不等式，$Q(a_t)<\hat Q(a_t)+\hat U(a_t)$成立的概率至少为$1-p$

实际上，在具体操作过程中，我们分母会加2处理，避免出现除0错。同时，可以设置p依旧随时间衰减。由此可见，两种方法的idea还是比较接近的。一般而言。为了更好地控制，我们可以选择$\max[\hat Q(a)+c\cdot \hat U(a)]$的那根拉杆。

## 汤普森采样算法

我们采用贝塔分布来给每一个选择的概率进行建模。我们根据目前已经建模的情况，进行一次随机采样。然后，我们选择采样结果最高的那一个。

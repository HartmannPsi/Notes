# 第一章 初探强化学习

强化学习，解决的是序贯决策的问题。目标是要找到一个最优的占用度量。占用度量，可以理解为在不同情况下，采取不同策略的概率分布模型。

# 第二章 多臂老虎机(MAB)问题

有一台老虎机，其有k根拉杆，每根拉杆对应不同的奖励概率分布。如何在操作T次拉杆后，获得尽量高的奖励呢？ 

我们引入懊悔(regret)这一概念来刻画拉动一根拉杆的期望奖励和最优拉杆期望之间的差距，定义为二者之差。我们的目标，就是要最小化懊悔。显然，我们需要不断拉一根拉杆，才能对它产生的值有一个大概的估计。我们使用增量式估计来更新，而非重新计算。递推的复杂度更低。

操作实际上可以分为两种类型：探索和利用。

## $\epsilon-$贪婪算法

每次有$1-\epsilon$的概率拉下期望奖励最大的拉杆，否则，就随机选择一根拉杆。$\epsilon$可以设置为随着时间下降。

## 上置信界算法

显然，如果我们对一根拉杆测试较多，那么它的确认度较高，我们更会相信目前的结果是可靠的。原理依靠于霍夫丁不等式。



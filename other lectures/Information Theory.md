# Lec 2

## 熵

记号约定：$X$是离散随机分布。$\mathcal{X}$为字母表。

我们用如下的方法定义熵：

$$
H(X)=-\sum_{x\in X}p(x)\log p(x)
$$

显然当$p(x)=0$时，不会提供任何信息。

一般此时$\log$的底数都取2。底数为2，称为bits。底数为e，称为nats。

$$
0\leq H(X)\leq\log|\mathcal{X}|
$$

第一个等号成立时，为不可能事件或者必然事件。

第二个等号成立时，为均一分布。

概率论的链式法则和贝叶斯规则：

$$
p(x_1,...,x_n)=p(x_n)p(x_{n-1}|x_n)...p(x_1|x_2,...,x_n)\\
p(x)p(y|x)=p(y)p(x|y)
$$

## 联合熵

对于多个随机变量的分布。

定义为：

$$
H(X,Y)=-\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log p(x,y)
$$

显然有：

$H(X,X)=H(X)$

$H(X,Y)=H(Y,X)$

## 条件熵

$X=x$已知，$p(Y=y|X=x)$也是一个概率密度函数。

$$
H(Y|X)=\sum_{x\in\mathcal{X}}p(x)H(Y|X=x)\\
=-\sum_{x\in\mathcal{X}}p(x)\sum_{y\in\mathcal{Y}}p(y|x)\log p(y|x)\\
=-\sum_{x\in\mathcal{X}}p(x,y)\sum_{y\in\mathcal{Y}}\log p(y|x)\\
=-E\log p(Y|X)
$$

从直觉的角度而言，条件熵已经有一定的限定了，所以有

$$
H(Y|X)\leq H(Y)\\
H(X|Y)\not=H(Y|X)\\
H(X|Y)+H(Y)=H(Y|X)+H(X)=H(X,Y)
$$

第三个等式是符合直觉的。

## 零熵

如果一个随机变量$X$相对另一个随机变量$Y$的条件熵为0，则$X$是$Y$的一个函数。这是符合直觉的。说明$X$的取值一直和$Y$有关。

# Lec 3

## 相对熵（K-L距离）

$$
D(p||q)=\sum_{x\in \mathcal{X}}p(x)\log {p(x)\over q(x)}\\
=E_p\log {p(X)\over q(X)}
$$

相对熵永远是非负的。

$$
D(p||q)=E_p\log p(X)-E_p\log q(X)
$$

容易观察到，相对熵并不是一种度量。首先，相对论显然就不具有对称性。

$$
D(p||q)=0\iff p=q
$$

Pinsker不等式 

$$
D(p||q)\geq{1\over 2\ln 2}V^2(p,q)
$$

## 互信息

$$
I(X;Y)=\sum_x\sum_yp(x,y)\log {p(x,y)\over p(x)p(y)}\\
=D(p(x,y)||p(x)p(y))\\
=E_{p(x,y)}\log{p(X,Y)\over p(X)p(Y)}
$$

 互信息符合对称性。$I(X,X)=H(X)$。这两部分都是符合直觉的。

可以用这样非正式的集合方法表示信息之间的关系：

$$
H(X,Y)=H(X)\cup H(Y)\\
I(X;Y)=H(X)\cap H(Y)\\
H(X|Y)=H(X)/I(X;Y)\\
H(Y|X)=H(Y)/I(X;Y)
$$

这样的相对关系，是非常符合直觉的。

$$
\log{p(X,Y)\over p(X)p(Y)}=-\log p(X)-\log p(Y)+\log p(X,Y)
$$

从而可以证明

$$
I(X;Y)=H(X)+H(Y)-H(X,Y)
$$

互信息可以在一定程度上刻画出变量之间的相关性。

## 熵的链式法则

$$
H(X_1,X_2,...,X_n)=H(X_1)+H(X_2|X_1)+...+H(X_n|X_{n-1},...,X_1)
$$

## 条件互信息

$$
I(X;Y|Z)=H(X,Z)-H(X|Y,Z)
$$

$$
I(X_1,...,X_n;Y)=\sum_{i=1}^nI(X_i;Y)
$$

## 条件相对熵

$$
D(p(y|x)||q(y|x))=E_{p(x,y)}\log {p(Y|X)\over q(Y|X)}
$$

$$
D(p(x,y)||q(x,y))=D(p(x)||q(x))+D(p(y|x)||q(y|x))
$$

# Lec 4

## 熵的独立界

$$
H(X_1,...,X_n)\leq \sum_{i=1}^nH(X_i)
$$

这一点是符合直觉的。等号成立当且仅当均为独立变量。

## 马尔科夫链

假设$X\to Y\to Z$构成一个马尔科夫链，则

$$
p(x,y,z)=p(x)p(y|x)p(z|y)
$$

马尔科夫过程应当是具有可逆性的。

$$
I(X;Z|Y)=0\\
I(X;Y|Z)=E_{p(X,Y,Z)}\log{p(X,Y|Z)\over p(X|Z)p(Y|Z)}
$$

### 数据处理不等式

$$
I(X;Y)\geq I(Y;Z)
$$

在如上马尔科夫链的假设上。

此外，我们可以定义：

$$
I(X;Y;Z)=I(X;Y)-I(X;Y|Z)
$$

## Fano不等式

我们打算去估计随机变量$X$的分布$p(x)$

我们观察到了与$X$相关的变量$Y$,概率分布为$p(y|x)$

$$
P_e=Pr(\hat X=X)\\
H(P_e)+P_e\log|\mathcal{X}|\geq H(X|\hat X)\geq H(X|Y)
$$

 我们定义误差的随机变量：

$$
E=0,\hat X=X;E=1,\hat X\not=X
$$

故，当$\hat X=X$之时，$P_e=0$,故在计算过程中可以略去这一项。

是故，我们有：

$$
H(P_e)+P_e\log(|\mathcal{X}-1|)\geq H(X|\hat X)
$$

# Lec 5

给出一系列随机变量$X_1,X_2,...$，我们称这个系列收敛于随机变量$X$，当以下三种条件其一符合时：

$$
\exist n,\forall \epsilon>0,Pr\{|X_n-X|>\epsilon\}\to0\\
\exist n,E(X_n-X)^2\to0\\
Pr\{\lim_{n\to\infin}X_n=X\}=1
$$

对于上述记号，当$A$是一个集合时，定义为：

$$
Pr\{X=A\}=\sum_{a\in A}Pr\{X=a\}
$$

大数定律：n个独立的随机变量$X_1,...X_n$符合$p(x) $

强大数定律：$Pr\{\lim_{n\to\infin}\bar X_n=E(X)\}=1$

弱大数定律：$\bar X_n\to E(X)$

## AEP（渐进均分性）

令$X_1,...,X_n$为$i.i.d$的随机变量，其分布均符合$p(x)$，则：

$$
-{1\over n}\log(p(X_1,...,X_n))=-{1\over n}\sum_{i=1}^n\log p(X_i)\to H(X)
$$

注意到上式的底数为2。取指数后，并取出随机变量的事件，我们就可以得到一些符合下面这个关系的事件：

$$
2^{-n(H(X)-\epsilon)}\leq p(x_1,...,x_n)\leq2^{-n(H(X)+\epsilon)}
$$

我们把这样的事件组成一个集合，称作典型集，记作$A_\epsilon^{(n)}$。典型集中的事件总数约为$2^{nH}$

典型集中事件发生的概率之和趋于1。

我们定义高概率集为最小的，满足$Pr(X=B_\delta^{(n)})\geq 1-\delta$的集合。

## 数据压缩

对数据首先进行编码，之后再进行解码。

定义$P_e=P(X^n\neq \hat X^n)$

# Lec 6

## 随机过程

如果n个随机变量之间存在相互联系呢？那么我们之后将引入熵率。

在随机过程中，各变量的观测是有顺序的。

eg：赌徒的破产，马尔科夫过程。即：

$$
X_{i+1}=X_i\underline+1
$$

### 稳态过程

$$
Pr\{X_1=x_1,...,X_n=x_n\}=Pr\{X_{1+l}=x_1,...,X_{n+l}=x_n\}
$$

也就是说，系统在时间上具有不变性。

故

$$
p(X_1)=p(X_2)=...=p(X_n)\\
p(X_1,X_3)=p(X_2,X_4)
$$

## 马尔科夫过程

$$
P(X_{n+1}=x_{n+1}|X_n=x_n,...,X_1=x_1)=Pr(X_{n+1}=x_{n+1}|X_n=x_n)
$$

马尔科夫链具有时间不变性。

当$p(X_{n+1})=p(X_n)$时，马尔科夫过程是一个稳态。

$$
p(x_{n+1})=\sum_{x_n}p(x_n)P_{x_nx_{n+1}}=x^TP=x^T
$$

## 熵率

对于一个有时序的随机过程，熵率可以这样定义：

$$
H(\mathcal{X})=\lim_{n\to\infin}{1\over n}H(X_1,...,X_n)
$$

由链式法则，

$$
H(X_n,...,X_1)=\sum_{i=1}^nH(X_i|X_{i-1},...,X_1)
$$

我们可以令右侧这个累加式中，每一项为$a_n$,随后验证其收敛性。

注意到，右侧这一条件熵每一项是递减的。并且，熵始终大于0。

定义：$H'(\mathcal{X})=\lim_{n\to\infin}H(X_n|X_{n-1},...,X_1)$

因此，熵率在数值上等于$H'(\mathcal{X})$

考虑马尔科夫过程的熵率。由于马尔科夫链具有无后效性，所有其熵率就是$H(X_2|X_1)$

## 热力学第二定律

我们可以把孤立系统看做是一个马尔科夫过程。

## 马尔科夫过程的函数

$X_1,...,X_n$是一个马尔科夫过程。

$Y_1,...,Y_n,Y_i=\phi(X_2)$

首先，我们可以找到条件熵的下界，也就是$H(\mathcal{Y})$

然后，我们可以找到$H(\mathcal{Y})$一个下界，也就是$H(Y_n|Y_{n-1},...,Y_1,X_1)$

两边同取极限即可。

# Lec 7

定义：编码的非奇异性。也就是编码和事件之间存在一个双射。

定义：前缀码，每一个编码都不是其他编码的前缀。

对一组前缀码，存在Kraft不等式：

$$
\sum_{i=1}^mD^{-i}\leq 1
$$

$D$是字母表的大小。$i$是每一个编码的长度。如果等号不成立，那么编码是有冗余的；如果等号成立，那么我们称其为完备码；若该等式不成立，那么相应的编码并不是唯一可解编码。

我们为什么需要前缀码？这是因为当传输信息时，我们不再需要按照固定长度（比如8个bit）来解码，而是可以直接从前往后，根据编码树解码。根据前缀码的性质，这样的解释是唯一的。Kraft不等式就在描述这样一件事情。

关于最优码，我们总是希望编码的平均长度越短越好。

## 香农编码

假设一个事件出现的概率为$p(i)$，我们令其对应事件的编码码长为$\lceil-\log p(i)\rceil$。从小到大进行编码。它也是前缀码，但不是最优码。只有当概率分布满足一定条件时，方可有紧致性。

# Lec 8

## Huffman编码

每次取出D个出现概率最低的编码，在当前位合并为一个编码，从而使期望编码最少。

为了Huffman编码最长编码情况出现的比较小，我们应当加入一些假符号，使其概率值为0，从而使总符号数为$1+k(D-1)$

我们称一个分布是D-adic的，如果每一个事件出现的概率均为$D^{-n},n\in N$

如果一个码字，没有兄弟姐妹，也就是说，其为父节点之唯一子节点，那么显然，我们可以删除最后一位。

## Shannon-Fano-Elias编码

事件无需按照发生概率的大小进行分布。我们定义一个累积概率函数，为该符号此前出现的所有发生事件的概率之和。然后，我们再在此基础上加上当前事件发生概率之一半。将此函数值表达为$D$进制之小数，把小数点后的后$\lceil p(x)\rceil+1$位作为其编码。

用数值来表示事件的idea。

# Lec 9

## 随机变量的生成

用二叉树来作为算法的表示树。每一个节点要么为叶子节点，要么就有两个儿子节点。

所有的事件都位于叶子节点处。在每一个节点处，向左儿子移动和向右儿子的概率是相等的。当每一个事件与叶子节点之间形成双射之时，树的期望深度与该随机变量的信息熵相等。如若不然，则树的期望深度不会小于信息熵的大小。

如果一个事件发生的概率并非$1\over 2$的正整数次幂，那么我们可以将其发生的概率改写为二进制，随后分解进行表示。

## 通用信源编码

在实际生活中，我们常常会不知道不同符号的概率分布，那么我们就没有办法按照概率来编码。

是故，在这个过程中，我们想要完成的事情，就是最小化我们估计的某一事件概率和该事件实际概率之间的差距。也就是最小化最大冗余。

冗余可以转化为信道的容量进行计算。

## 算术编码

算术编码依旧不能脱出需要已知概率的桎梏。它承袭了Shannon-Fanos-Elias编码的思想。首先，其将$[0,1)$按照概率之分布，划分为对应之区间。倘若某一数字在某符号对应之区间中，则说明这一位便是它了。此后，再将每一区间按概率层层划分，依此法进行划分，从而解码得到结果。

## LZ算法

此为一种无损压缩之方式。它使用了一个滑动窗口进行编码，在这个滑动窗口中，包含两部分内容。第一部分是已编码部分，称为查找缓冲区。第二部分是未编码部分，称为先行缓冲区。然后，在查找缓冲区中检索，是否有先行缓冲区中的第一个字符。然后，我们用一个三元组来表示之。三元组中的内容，包含偏移量（从查找缓冲区的哪里复制？）、长度（需要复制多少个字符？）、后继（下一个字符是什么？）。倘若没有搜索到，那么记录为$<0,0,C(...)>$，…代表的就是下一个字符。

显然，需要滑动窗口的大小是相同的，这样才可以保证其正确性。
